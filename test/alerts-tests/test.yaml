rule_files:
- rules.yaml
evaluation_interval: 1m
tests:
- interval: 1m
  input_series:
  - series: kube_node_status_condition{condition="Ready", status="true"}
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: KubernetesNodeReady
    exp_alerts:
    - exp_labels:
        severity: critical
        condition: Ready
        status: true
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Node ready (instance )"
        description: "Node  has been unready for a long time\n  VALUE = 0\n  LABELS: map[__name__:kube_node_status_condition alertgroup:KubernetesAlerts alertname:KubernetesNodeReady condition:Ready group_name:KubernetesAlerts severity:critical status:true]"

- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="Ready", status="true"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesNodeReady
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="MemoryPressure", status="true"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesMemoryPressure
    exp_alerts:
    - exp_labels:
        severity: critical
        condition: MemoryPressure
        status: true
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes memory pressure (instance )"
        description: " has MemoryPressure condition\n  VALUE = 1\n  LABELS: map[__name__:kube_node_status_condition alertgroup:KubernetesAlerts alertname:KubernetesMemoryPressure condition:MemoryPressure group_name:KubernetesAlerts severity:critical status:true]"

- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="MemoryPressure", status="true"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesMemoryPressure
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="DiskPressure", status="true"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDiskPressure
    exp_alerts:
    - exp_labels:
        severity: critical
        condition: DiskPressure
        status: true
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes disk pressure (instance )"
        description: " has DiskPressure condition\n  VALUE = 1\n  LABELS: map[__name__:kube_node_status_condition alertgroup:KubernetesAlerts alertname:KubernetesDiskPressure condition:DiskPressure group_name:KubernetesAlerts severity:critical status:true]"

- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="DiskPressure", status="true"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDiskPressure
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="OutOfDisk", status="true"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesOutOfDisk
    exp_alerts:
    - exp_labels:
        severity: critical
        condition: OutOfDisk
        status: true
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes out of disk (instance )"
        description: " has OutOfDisk condition\n  VALUE = 1\n  LABELS: map[__name__:kube_node_status_condition alertgroup:KubernetesAlerts alertname:KubernetesOutOfDisk condition:OutOfDisk group_name:KubernetesAlerts severity:critical status:true]"
        
- interval: 1m
  input_series:
  - series: 'kube_node_status_condition{condition="OutOfDisk", status="true"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesOutOfDisk
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_job_status_failed'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesJobFailed
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Job failed (instance )"
        description: "Job / failed to complete\n  VALUE = 1\n  LABELS: map[__name__:kube_job_status_failed alertgroup:KubernetesAlerts alertname:KubernetesJobFailed group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_job_status_failed'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesJobFailed
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_cronjob_spec_suspend'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesCronjobSuspended
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes CronJob suspended (instance )"
        description: "CronJob / is suspended\n  VALUE = 1\n  LABELS: map[__name__:kube_cronjob_spec_suspend alertgroup:KubernetesAlerts alertname:KubernetesCronjobSuspended group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_cronjob_spec_suspend'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesCronjobSuspended
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_persistentvolumeclaim_status_phase{phase="Pending"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesPersistentvolumeclaimPending
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
        phase: Pending
      exp_annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance )"
        description: "PersistentVolumeClaim / is pending\n  VALUE = 1\n  LABELS: map[__name__:kube_persistentvolumeclaim_status_phase alertgroup:KubernetesAlerts alertname:KubernetesPersistentvolumeclaimPending group_name:KubernetesAlerts phase:Pending severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_persistentvolumeclaim_status_phase{phase="Pending"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesPersistentvolumeclaimPending
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_persistentvolume_status_phase{phase="Pending",job="kube-state-metrics"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesPersistentvolumeError
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
        phase: Pending
        job: kube-state-metrics
      exp_annotations:
        summary: "Kubernetes PersistentVolume error (instance )"
        description: "Persistent volume is in bad state\n  VALUE = 1\n  LABELS: map[__name__:kube_persistentvolume_status_phase alertgroup:KubernetesAlerts alertname:KubernetesPersistentvolumeError group_name:KubernetesAlerts job:kube-state-metrics phase:Pending severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_persistentvolume_status_phase{phase="Pending",job="kube-state-metrics"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesPersistentvolumeError
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes'
    values: "24x1440"
  - series: 'kubelet_volume_stats_capacity_bytes'
    values: "100x1440"  
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesVolumeOutOfDiskSpaceWarning
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Volume out of disk space (instance )"
        description: "Volume is almost full (< 25 percent left)\n  VALUE = 24\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesVolumeOutOfDiskSpaceWarning group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes'
    values: "50x1440"
  - series: 'kubelet_volume_stats_capacity_bytes'
    values: "100x1440"  
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesVolumeOutOfDiskSpaceWarning
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes'
    values: "9x1440"
  - series: 'kubelet_volume_stats_capacity_bytes'
    values: "100x1440"  
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesVolumeOutOfDiskSpaceHigh
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Volume out of disk space (instance )"
        description: "Volume is almost full (< 10 percent left)\n  VALUE = 9\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesVolumeOutOfDiskSpaceHigh group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes'
    values: "50x1440"
  - series: 'kubelet_volume_stats_capacity_bytes'
    values: "100x1440"  
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesVolumeOutOfDiskSpaceHigh
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes'
    values: "-1x420"
  alert_rule_test:
  - eval_time: 7h
    groupname: KubernetesAlerts
    alertname: KubernetesVolumeFullInFourDays
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Volume full in four days (instance )"
        description: "/ is expected to fill up within four days. Currently -1 percent is available.\n  VALUE = -1\n  LABELS: map[__name__:kubelet_volume_stats_available_bytes alertgroup:KubernetesAlerts alertname:KubernetesVolumeFullInFourDays group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes'
    values: "50x420"
  alert_rule_test:
  - eval_time: 7h
    groupname: KubernetesAlerts
    alertname: KubernetesVolumeFullInFourDays
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_statefulset_replicas'
    values: "2x1440"
  - series: 'kube_statefulset_status_replicas_ready'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetDown
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes StatefulSet down (instance )"
        description: "A StatefulSet went down\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesStatefulsetDown group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_statefulset_replicas'
    values: "2x1440"
  - series: 'kube_statefulset_status_replicas_ready'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetDown
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_pod_status_phase{phase="Pending"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesPodNotHealthy
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Pod not healthy (instance )"
        description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesPodNotHealthy group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_pod_status_phase{phase="Pending"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesPodNotHealthy
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_pod_container_status_restarts_total'
    values: "1+2x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: KubernetesPodCrashLooping
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes pod crash looping (instance )"
        description: "Pod  is crash looping\n  VALUE = 10\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesPodCrashLooping group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_pod_container_status_restarts_total'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: KubernetesPodCrashLooping
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_replicaset_spec_replicas'
    values: "2x1440"
  - series: 'kube_replicaset_status_ready_replicas'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesReplicassetMismatch
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes ReplicasSet mismatch (instance )"
        description: "Deployment Replicas mismatch\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesReplicassetMismatch group_name:KubernetesAlerts severity:warning]"        

- interval: 1m
  input_series:
  - series: 'kube_replicaset_spec_replicas'
    values: "2x1440"
  - series: 'kube_replicaset_status_ready_replicas'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesReplicassetMismatch
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_deployment_spec_replicas'
    values: "2x1440"
  - series: 'kube_deployment_status_replicas_available'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDeploymentReplicasMismatch
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Deployment replicas mismatch (instance )"
        description: "Deployment Replicas mismatch\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesDeploymentReplicasMismatch group_name:KubernetesAlerts severity:warning]"      

- interval: 1m
  input_series:
  - series: 'kube_deployment_spec_replicas'
    values: "2x1440"
  - series: 'kube_deployment_status_replicas_available'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDeploymentReplicasMismatch
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_replicas_ready'
    values: "2x1440"
  - series: 'kube_statefulset_status_replicas'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetReplicasMismatch
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes StatefulSet replicas mismatch (instance )"
        description: "A StatefulSet has not matched the expected number of replicas for longer than 15 minutes.\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesStatefulsetReplicasMismatch group_name:KubernetesAlerts severity:warning]" 

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_replicas_ready'
    values: "2x1440"
  - series: 'kube_statefulset_status_replicas'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetReplicasMismatch
    exp_alerts: [] 

- interval: 1m
  input_series:
  - series: 'kube_deployment_status_observed_generation'
    values: "2x1440"
  - series: 'kube_deployment_metadata_generation'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDeploymentGenerationMismatch
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes Deployment generation mismatch (instance )"
        description: "A Deployment has failed but has not been rolled back.\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesDeploymentGenerationMismatch group_name:KubernetesAlerts severity:critical]" 

- interval: 1m
  input_series:
  - series: 'kube_deployment_status_observed_generation'
    values: "2x1440"
  - series: 'kube_deployment_metadata_generation'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDeploymentGenerationMismatch
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_observed_generation'
    values: "2x1440"
  - series: 'kube_statefulset_metadata_generation'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetGenerationMismatch
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes StatefulSet generation mismatch (instance )"
        description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesStatefulsetGenerationMismatch group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_observed_generation'
    values: "2x1440"
  - series: 'kube_statefulset_metadata_generation'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetGenerationMismatch
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_current_revision'
    values: "1x1440"
  - series: 'kube_statefulset_replicas'
    values: "1x1440"
  - series: 'kube_statefulset_status_replicas_updated'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetUpdateNotRolledOut
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes StatefulSet update not rolled out (instance )"
        description: "StatefulSet update has not been rolled out.\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesStatefulsetUpdateNotRolledOut group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_current_revision'
    values: "1x1440"
  - series: 'kube_statefulset_status_update_revision'
    values: "1x1440"  
  - series: 'kube_statefulset_replicas'
    values: "1x1440"
  - series: 'kube_statefulset_status_replicas_updated'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesStatefulsetUpdateNotRolledOut
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_daemonset_status_number_ready'
    values: "1x1440"
  - series: 'kube_daemonset_status_desired_number_scheduled'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDaemonsetRolloutStuck
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes DaemonSet rollout stuck (instance )"
        description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = 50\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesDaemonsetRolloutStuck group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_daemonset_status_number_ready'
    values: "2x1440"
  - series: 'kube_daemonset_status_desired_number_scheduled'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDaemonsetRolloutStuck
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_daemonset_status_number_misscheduled'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDaemonsetMisscheduled
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes DaemonSet misscheduled (instance )"
        description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = 1\n  LABELS: map[__name__:kube_daemonset_status_number_misscheduled alertgroup:KubernetesAlerts alertname:KubernetesDaemonsetMisscheduled group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_daemonset_status_number_misscheduled'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesDaemonsetMisscheduled
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_cronjob_next_schedule_time'
    values: "-3700x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesCronjobTooLong
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes CronJob too long (instance )"
        description: "CronJob / is taking more than 1h to complete.\n  VALUE = 4000\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesCronjobTooLong group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_cronjob_next_schedule_time'
    values: "3700x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesCronjobTooLong
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_cronjob_next_schedule_time'
    values: "-3700x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesCronjobTooLong
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes CronJob too long (instance )"
        description: "CronJob / is taking more than 1h to complete.\n  VALUE = 4000\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesCronjobTooLong group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_cronjob_next_schedule_time'
    values: "3700x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesCronjobTooLong
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_job_status_failed'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesJobCompletion
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes job completion (instance )"
        description: "Kubernetes Job failed to complete\n  VALUE = 1\n  LABELS: map[__name__:kube_job_status_failed alertgroup:KubernetesAlerts alertname:KubernetesJobCompletion group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'kube_job_status_failed'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: KubernetesAlerts
    alertname: KubernetesJobCompletion
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'apiserver_request_count{job="kube-apiserver",code="500"}'
    values: "1+1x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: KubernetesApiServerErrors
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes API server errors (instance )"
        description: "Kubernetes API server is experiencing high error rate\n  VALUE = 100\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesApiServerErrors group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'apiserver_request_count{job="kube-apiserver",code="500"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: KubernetesApiServerErrors
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'apiserver_request_duration_seconds_bucket{le="0.1"}'
    values: "0+10x1440"
  - series: 'apiserver_request_duration_seconds_bucket{le="0.3"}'
    values: "0+20x1440"
  - series: 'apiserver_request_duration_seconds_bucket{le="0.5"}'
    values: "0+30x1440"
  - series: 'apiserver_request_duration_seconds_bucket{le="1"}'
    values: "0+100x1440"    
  - series: 'apiserver_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+110x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: ApiServerRequestsSlow
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "API Server requests are slow(instance )"
        description: "HTTP requests slowing down, 99th quantile is over 0.5s for 5 minutes\\n  VALUE = 1\n  LABELS: map[alertgroup:KubernetesAlerts alertname:ApiServerRequestsSlow group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'apiserver_request_duration_seconds_bucket{le="0.1"}'
    values: "0+100x1440"
  - series: 'apiserver_request_duration_seconds_bucket{le="0.2"}'
    values: "0+101x1440"
  - series: 'apiserver_request_duration_seconds_bucket{le="0.3"}'
    values: "0+102x1440"
  - series: 'apiserver_request_duration_seconds_bucket{le="0.4"}'
    values: "0+103x1440"    
  - series: 'apiserver_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+104x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: ApiServerRequestsSlow
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'workqueue_depth'
    values: "11x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: ControllerWorkQueueDepth
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Controller work queue depth is more than 10 (instance )"
        description: "Controller work queue depth is more than 10\n  VALUE = 11\n  LABELS: map[alertgroup:KubernetesAlerts alertname:ControllerWorkQueueDepth group_name:KubernetesAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'workqueue_depth'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: ControllerWorkQueueDepth
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'rest_client_requests_total{code="400"}'
    values: "1+1x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: KubernetesApiClientErrors
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
      exp_annotations:
        summary: "Kubernetes API client errors (instance )"
        description: "Kubernetes API client is experiencing high error rate\n  VALUE = 100\n  LABELS: map[alertgroup:KubernetesAlerts alertname:KubernetesApiClientErrors group_name:KubernetesAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'rest_client_requests_total{code="400"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: KubernetesAlerts
    alertname: KubernetesApiClientErrors
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'apiserver_client_certificate_expiration_seconds_bucket {job="kubelet", le="0.1"}'
    values: "0+10x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="0.3"}'
    values: "0+20x1440"
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet", le="0.5"}'
    values: "0+30x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="1"}'
    values: "0+100x1440"    
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet",le="+Inf"}'
    values: "0+110x1440"
  - series: 'apiserver_client_certificate_expiration_seconds_count{job="kubelet"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: KubernetesClientCertificateExpiresNextWeek
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: KubernetesAlerts
        job: kubelet
      exp_annotations:
        summary: "Kubernetes client certificate expires next week (instance )"
        description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = 1\n  LABELS: map[__name__:apiserver_client_certificate_expiration_seconds_count alertgroup:KubernetesAlerts alertname:KubernetesClientCertificateExpiresNextWeek group_name:KubernetesAlerts job:kubelet severity:warning]"

- interval: 1m
  input_series:
  - series: 'apiserver_client_certificate_expiration_seconds_bucket {job="kubelet", le="0.1"}'
    values: "0+10x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="0.3"}'
    values: "0+20x1440"
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet", le="0.5"}'
    values: "0+30x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="1"}'
    values: "0+100x1440"    
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet",le="+Inf"}'
    values: "0+110x1440"
  - series: 'apiserver_client_certificate_expiration_seconds_count{job="kubelet"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: KubernetesClientCertificateExpiresNextWeek
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'apiserver_client_certificate_expiration_seconds_bucket {job="kubelet", le="0.1"}'
    values: "0+10x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="0.3"}'
    values: "0+20x1440"
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet", le="0.5"}'
    values: "0+30x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="1"}'
    values: "0+100x1440"    
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet",le="+Inf"}'
    values: "0+110x1440"
  - series: 'apiserver_client_certificate_expiration_seconds_count{job="kubelet"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: KubernetesClientCertificateExpiresSoon
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: KubernetesAlerts
        job: kubelet
      exp_annotations:
        summary: "Kubernetes client certificate expires soon (instance )"
        description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = 1\n  LABELS: map[__name__:apiserver_client_certificate_expiration_seconds_count alertgroup:KubernetesAlerts alertname:KubernetesClientCertificateExpiresSoon group_name:KubernetesAlerts job:kubelet severity:critical]"

- interval: 1m
  input_series:
  - series: 'apiserver_client_certificate_expiration_seconds_bucket {job="kubelet", le="0.1"}'
    values: "0+10x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="0.3"}'
    values: "0+20x1440"
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet", le="0.5"}'
    values: "0+30x1440"
  - series: 'apiserver_request_duration_seconds_bucket{job="kubelet", le="1"}'
    values: "0+100x1440"    
  - series: 'apiserver_request_duration_seconds_bucket {job="kubelet",le="+Inf"}'
    values: "0+110x1440"
  - series: 'apiserver_client_certificate_expiration_seconds_count{job="kubelet"}'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: KubernetesAlerts
    alertname: KubernetesClientCertificateExpiresSoon
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'container_processes'
    values: "9x1440"
  - series: 'node_processes_max_processes'
    values: "10x1440"
  - series: 'node_processes_threads'
    values: "1x1440"    
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeProcesses
    alertname: CountPidsAndThreadOutOfLimit
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeProcesses
      exp_annotations:
        summary: "Host high PIDs and Threads usage (instance )"
        description: "Sum of node's pids and threads is filling up (< 20 percent left)\n  VALUE = 100\n  LABELS: map[alertgroup:NodeProcesses alertname:CountPidsAndThreadOutOfLimit group_name:NodeProcesses severity:warning]"

- interval: 1m
  input_series:
  - series: 'container_processes'
    values: "1x1440"
  - series: 'node_processes_max_processes'
    values: "100x1440"
  - series: 'node_processes_threads'
    values: "10x1440"    
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeProcesses
    alertname: CountPidsAndThreadOutOfLimit
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_filesystem_size_bytes{fstype="ext"}'
    values: "100x1440"
  - series: 'node_filesystem_free_bytes{fstype="ext"}'
    values: "0x1440"
  - series: 'node_filesystem_avail_bytes{fstype="ext"}'
    values: "25x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeExporters
    alertname: NodeDiskUsageIsMoreThanWarningThreshold
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
        fstype: ext
      exp_annotations:
        summary: "Disk usage on node > 70 percent (instance )"
        description: "Node  disk usage of  is\n  VALUE = 80 percent"
        
- interval: 1m
  input_series:
  - series: 'node_filesystem_size_bytes{fstype="ext"}'
    values: "100x1440"
  - series: 'node_filesystem_free_bytes{fstype="ext"}'
    values: "100x1440"
  - series: 'node_filesystem_avail_bytes{fstype="ext"}'
    values: "100x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeExporters
    alertname: NodeDiskUsageIsMoreThanWarningThreshold
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'node_filesystem_size_bytes{fstype="ext"}'
    values: "100x1440"
  - series: 'node_filesystem_free_bytes{fstype="ext"}'
    values: "1x1440"
  - series: 'node_filesystem_avail_bytes{fstype="ext"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeExporters
    alertname: NodeDiskUsageIsMoreThanCriticalThreshold
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: NodeExporters
        fstype: ext
      exp_annotations:
        summary: "Disk usage on node > 90 percent (instance )"
        description: "Node  disk usage of  is\n VALUE = 99 percent"

- interval: 1m
  input_series:
  - series: 'node_filesystem_size_bytes{fstype="ext"}'
    values: "100x1440"
  - series: 'node_filesystem_free_bytes{fstype="ext"}'
    values: "100x1440"
  - series: 'node_filesystem_avail_bytes{fstype="ext"}'
    values: "100x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeExporters
    alertname: NodeDiskUsageIsMoreThanCriticalThreshold
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_memory_MemAvailable_bytes'
    values: "1x1440"
  - series: 'node_memory_MemTotal_bytes'
    values: "100x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 15m
    groupname: NodeExporters
    alertname: HostOutOfMemory
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host out of memory (instance )"
        description: "Node memory is filling up (< 10 percent left)\n  VALUE = 1\n  LABELS: map[alertgroup:NodeExporters alertname:HostOutOfMemory group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_memory_MemAvailable_bytes'
    values: "100x1440"
  - series: 'node_memory_MemTotal_bytes'
    values: "100x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 15m
    groupname: NodeExporters
    alertname: HostOutOfMemory
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_vmstat_pgmajfault'
    values: "120000+120000x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostMemoryUnderMemoryPressure
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host memory under memory pressure (instance )"
        description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = 2000\n  LABELS: map[alertgroup:NodeExporters alertname:HostMemoryUnderMemoryPressure group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_vmstat_pgmajfault'
    values: "0x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostMemoryUnderMemoryPressure
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_network_receive_bytes_total'
    values: "12582912000+12582912000x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualNetworkThroughputIn
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host unusual network throughput in (instance )"
        description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = 200\n  LABELS: map[alertgroup:NodeExporters alertname:HostUnusualNetworkThroughputIn group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_network_receive_bytes_total'
    values: "0x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualNetworkThroughputIn
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_network_transmit_bytes_total'
    values: "12582912000+12582912000x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualNetworkThroughputOut
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host unusual network throughput out (instance )"
        description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = 200\n  LABELS: map[alertgroup:NodeExporters alertname:HostUnusualNetworkThroughputOut group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_network_transmit_bytes_total'
    values: "0x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualNetworkThroughputOut
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_disk_read_bytes_total'
    values: "12582912000+12582912000x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualDiskReadRate
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host unusual disk read rate (instance )"
        description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = 200\n  LABELS: map[alertgroup:NodeExporters alertname:HostUnusualDiskReadRate group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_disk_read_bytes_total'
    values: "0x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualDiskReadRate
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_disk_written_bytes_total'
    values: "12582912000+12582912000x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualDiskWriteRate
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host unusual disk write rate (instance )"
        description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = 200\n  LABELS: map[alertgroup:NodeExporters alertname:HostUnusualDiskWriteRate group_name:NodeExporters severity:warning]"
        
- interval: 1m
  input_series:
  - series: 'node_disk_written_bytes_total'
    values: "0x1440"
  - series: 'node_uname_info'
    values: "1x1440"   
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostUnusualDiskWriteRate
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_filesystem_avail_bytes{mountpoint="/"}'
    values: "5x1440"   
  - series: 'node_filesystem_size_bytes{mountpoint="/"}'
    values: "100x1440"
  - series: 'node_uname_info'
    values: "1x1440"    
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostOutOfDiskSpace
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
        mountpoint: /
      exp_annotations:
        summary: "Host out of disk space (instance )"
        description: "Disk is almost full (< 10 percent left)\n  VALUE = 5\n  LABELS: map[alertgroup:NodeExporters alertname:HostOutOfDiskSpace group_name:NodeExporters mountpoint:/ severity:warning]"
        
- interval: 1m
  input_series:
  - series: 'node_filesystem_avail_bytes{mountpoint="/"}'
    values: "100x1440"   
  - series: 'node_filesystem_size_bytes{mountpoint="/"}'
    values: "100x1440"
  - series: 'node_uname_info'
    values: "1x1440"    
  alert_rule_test:
  - eval_time: 6m
    groupname: NodeExporters
    alertname: HostOutOfDiskSpace
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_filesystem_free_bytes'
    values: "-1x300"   
  - series: 'node_uname_info'
    values: "1x300"   
  alert_rule_test:
  - eval_time: 5h
    groupname: NodeExporters
    alertname: HostDiskWillFillIn4Hours
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host disk will fill in 4 hours (instance )"
        description: "Disk will fill in 4 hours at current write rate\n  VALUE = -1\n  LABELS: map[alertgroup:NodeExporters alertname:HostDiskWillFillIn4Hours group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_filesystem_free_bytes'
    values: "1x300"   
  - series: 'node_uname_info'
    values: "1x300"   
  alert_rule_test:
  - eval_time: 5h
    groupname: NodeExporters
    alertname: HostDiskWillFillIn4Hours
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_filesystem_files_free{mountpoint ="/"}'
    values: "1x1440"
  - series: 'node_filesystem_files{mountpoint ="/"}'
    values: "100x1440"
  - series: 'node_uname_info'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeExporters
    alertname: HostOutOfInodes
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
        mountpoint: /
      exp_annotations:
        summary: "Host out of inodes (instance )"
        description: "Disk is almost running out of available inodes (< 10 percent left)\n  VALUE = 1\n  LABELS: map[alertgroup:NodeExporters alertname:HostOutOfInodes group_name:NodeExporters mountpoint:/ severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_filesystem_files_free{mountpoint ="/"}'
    values: "100x1440"
  - series: 'node_filesystem_files{mountpoint ="/"}'
    values: "100x1440"
  - series: 'node_uname_info'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: NodeExporters
    alertname: HostOutOfInodes
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_disk_read_time_seconds_total'
    values: "24000+24000x1440"   
  - series: 'node_disk_reads_completed_total'
    values: "120+120x1440"
  - series: 'node_uname_info'
    values: "1x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: NodeExporters
    alertname: HostUnusualDiskReadLatency
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host unusual disk read latency (instance )"
        description: "Disk latency is growing (read operations > 100ms)\n  VALUE = 200\n  LABELS: map[alertgroup:NodeExporters alertname:HostUnusualDiskReadLatency group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_disk_read_time_seconds_total'
    values: "240+240x1440"   
  - series: 'node_disk_reads_completed_total'
    values: "120+120x1440"
  - series: 'node_uname_info'
    values: "1x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: NodeExporters
    alertname: HostUnusualDiskReadLatency
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_disk_write_time_seconds_total'
    values: "24000+24000x1440"   
  - series: 'node_disk_writes_completed_total'
    values: "120+120x1440"
  - series: 'node_uname_info'
    values: "1x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: NodeExporters
    alertname: HostUnusualDiskWriteLatency
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host unusual disk write latency (instance )"
        description: "Disk latency is growing (write operations > 100ms)\n  VALUE = 200\n  LABELS: map[alertgroup:NodeExporters alertname:HostUnusualDiskWriteLatency group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_disk_write_time_seconds_total'
    values: "240+240x1440"   
  - series: 'node_disk_writes_completed_total'
    values: "120+120x1440"
  - series: 'node_uname_info'
    values: "1x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: NodeExporters
    alertname: HostUnusualDiskWriteLatency
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'node_cpu_seconds_total{mode="idle"}'
    values: "0x1440"
  - series: 'node_uname_info'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 1h
    groupname: NodeExporters
    alertname: HostHighCpuLoad
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NodeExporters
      exp_annotations:
        summary: "Host high CPU load (instance )"
        description: "CPU load is > 80 percent\n  VALUE = 100\n  LABELS: map[alertgroup:NodeExporters alertname:HostHighCpuLoad group_name:NodeExporters severity:warning]"

- interval: 1m
  input_series:
  - series: 'node_cpu_seconds_total{mode="idle"}'
    values: "300+300x1440"
  - series: 'node_uname_info'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 1h
    groupname: NodeExporters
    alertname: HostHighCpuLoad
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'container_last_seen'
    values: "-61x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DockerContainers
    alertname: ContainerKilled
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: DockerContainers
      exp_annotations:
        summary: "Container killed (instance )"
        description: "A container has disappeared\n  VALUE = 361\n  LABELS: map[alertgroup:DockerContainers alertname:ContainerKilled group_name:DockerContainers severity:warning]"

- interval: 1m
  input_series:
  - series: 'container_last_seen'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DockerContainers
    alertname: ContainerKilled
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'container_fs_inodes_free'
    values: "15x1440"
  - series: 'container_fs_inodes_total'
    values: "100x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DockerContainers
    alertname: ContainerVolumeUsage
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: DockerContainers
      exp_annotations:
        summary: "Container Volume usage (instance )"
        description: "Container Volume usage is above 80 percent\n  VALUE = 85\n  LABELS: map[alertgroup:DockerContainers alertname:ContainerVolumeUsage group_name:DockerContainers severity:warning]"

- interval: 1m
  input_series:
  - series: 'container_fs_inodes_free'
    values: "100x1440"
  - series: 'container_fs_inodes_total'
    values: "100x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DockerContainers
    alertname: ContainerVolumeUsage
    exp_alerts: []

        
- interval: 1m
  input_series:
  - series: 'container_fs_io_current'
    values: "0.85x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DockerContainers
    alertname: ContainerVolumeIoUsage
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: DockerContainers
      exp_annotations:
        summary: "Container Volume IO usage (instance )"
        description: "Container Volume IO usage is above 80 percent\n  VALUE = 85\n  LABELS: map[alertgroup:DockerContainers alertname:ContainerVolumeIoUsage group_name:DockerContainers severity:warning]"

- interval: 1m
  input_series:
  - series: 'container_fs_io_current'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DockerContainers
    alertname: ContainerVolumeIoUsage
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'container_cpu_cfs_throttled_seconds_total'
    values: "120+120x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: DockerContainers
    alertname: ContainerHighThrottleRate
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: DockerContainers
      exp_annotations:
        summary: "Container high throttle rate (instance )"
        description: "Container is being throttled\n  VALUE = 2\n  LABELS: map[alertgroup:DockerContainers alertname:ContainerHighThrottleRate group_name:DockerContainers severity:warning]"

- interval: 1m
  input_series:
  - series: 'container_cpu_cfs_throttled_seconds_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: DockerContainers
    alertname: ContainerHighThrottleRate
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_deployment_status_replicas_available'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesDeploymentAvailableReplicas
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAmode
      exp_annotations:
        summary: "Not HA mode: Deployment Available Replicas < 2 (instance )"
        description: "Not HA mode: Kubernetes Deployment has less than 2 available replicas\n  VALUE = 1\n  LABELS: map[__name__:kube_deployment_status_replicas_available alertgroup:HAmode alertname:NotHAKubernetesDeploymentAvailableReplicas group_name:HAmode severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_deployment_status_replicas_available'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesDeploymentAvailableReplicas
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_replicas_available'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesStatefulSetAvailableReplicas
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAmode
      exp_annotations:
        summary: "Not HA mode: StatefulSet Available Replicas < 2 (instance )"
        description: "Not HA mode: Kubernetes StatefulSet has less than 2 available replicas\n  VALUE = 1\n  LABELS: map[__name__:kube_statefulset_status_replicas_available alertgroup:HAmode alertname:NotHAKubernetesStatefulSetAvailableReplicas group_name:HAmode severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_replicas_available'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesStatefulSetAvailableReplicas
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_deployment_status_replicas'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesDeploymentDesiredReplicas
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAmode
      exp_annotations:
        summary: "Not HA mode: Deployment Desired Replicas < 2 (instance )"
        description: "Not HA mode: Kubernetes Deployment has less than 2 desired replicas\n  VALUE = 1\n  LABELS: map[__name__:kube_deployment_status_replicas alertgroup:HAmode alertname:NotHAKubernetesDeploymentDesiredReplicas group_name:HAmode severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_deployment_status_replicas'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesDeploymentDesiredReplicas
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_replicas'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesStatefulSetDesiredReplicas
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAmode
      exp_annotations:
        summary: "Not HA mode: StatefulSet Desired Replicas < 2 (instance )"
        description: "Not HA mode: Kubernetes StatefulSet has less than 2 desired replicas\n  VALUE = 1\n  LABELS: map[__name__:kube_statefulset_status_replicas alertgroup:HAmode alertname:NotHAKubernetesStatefulSetDesiredReplicas group_name:HAmode severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_statefulset_status_replicas'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesStatefulSetDesiredReplicas
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'kube_pod_info{namespace="test", node="test", created_by_kind="ReplicaSet", created_by_name="test"}'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesDeploymentMultiplePodsPerNode
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAmode
      exp_annotations:
        summary: "Not HA mode: Deployment Has Multiple Pods per Node (instance )"
        description: "Not HA mode: Kubernetes Deployment has 2 or more replicas on the same node\n  VALUE = 1\n  LABELS: map[alertgroup:HAmode alertname:NotHAKubernetesDeploymentMultiplePodsPerNode group_name:HAmode severity:warning]"   

- interval: 1m
  input_series:
  - series: 'kube_pod_info{namespace="test", node="test", created_by_kind="ReplicaSet", created_by_name="test"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesDeploymentMultiplePodsPerNode
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'kube_pod_info{namespace="test", node="test", created_by_kind="StatefulSet", created_by_name="test"}'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesStatefulSetMultiplePodsPerNode
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAmode
      exp_annotations:
        summary: "Not HA mode: StatefulSet Has Multiple Pods per Node (instance )"
        description: "Not HA mode: Kubernetes StatefulSet has 2 or more replicas on the same node\n  VALUE = 1\n  LABELS: map[alertgroup:HAmode alertname:NotHAKubernetesStatefulSetMultiplePodsPerNode group_name:HAmode severity:warning]"

- interval: 1m
  input_series:
  - series: 'kube_pod_info{namespace="test", node="test", created_by_kind="StatefulSet", created_by_name="test"}'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAmode
    alertname: NotHAKubernetesStatefulSetMultiplePodsPerNode
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_up'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyDown
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy down (instance )"
        description: "HAProxy down\n  VALUE = 0\n  LABELS: map[__name__:haproxy_up alertgroup:HAproxy alertname:HaproxyDown group_name:HAproxy severity:critical]"
        
- interval: 1m
  input_series:
  - series: 'haproxy_up'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyDown
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_backend_connection_errors_total'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: HAproxy
    alertname: HaproxyBackendConnectionErrors
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy backend connection errors (instance )"
        description: "Too many connection errors to / backend (> 10 req/s). Request throughput may be to high.\n  VALUE = 20\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyBackendConnectionErrors group_name:HAproxy severity:critical]"

- interval: 1m
  input_series:
  - series: 'haproxy_backend_connection_errors_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: HAproxy
    alertname: HaproxyBackendConnectionErrors
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_server_response_errors_total'
    values: "600+600x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: HAproxy
    alertname: HaproxyServerResponseErrors
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy server response errors (instance )"
        description: "Too many response errors to  server (> 5 req/s).\n  VALUE = 10\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyServerResponseErrors group_name:HAproxy severity:critical]"

- interval: 1m
  input_series:
  - series: 'haproxy_server_response_errors_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: HAproxy
    alertname: HaproxyServerResponseErrors
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_server_connection_errors_total'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: HAproxy
    alertname: HaproxyServerConnectionErrors
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy server connection errors (instance )"
        description: "Too many connection errors to  server (> 10 req/s). Request throughput may be to high.\n  VALUE = 20\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyServerConnectionErrors group_name:HAproxy severity:critical]"

- interval: 1m
  input_series:
  - series: 'haproxy_server_connection_errors_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: HAproxy
    alertname: HaproxyServerConnectionErrors
    exp_alerts: []
     
- interval: 1m
  input_series:
  - series: 'haproxy_backend_current_queue'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyPendingRequests
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy pending requests (instance )"
        description: "Some HAProxy requests are pending on / backend\n  VALUE = 1\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyPendingRequests group_name:HAproxy severity:warning]"

- interval: 1m
  input_series:
  - series: 'haproxy_backend_current_queue'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyPendingRequests
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_backend_http_total_time_average_seconds'
    values: "3x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyHttpSlowingDown
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy HTTP slowing down (instance )"
        description: "Average request time is increasing\n  VALUE = 3\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyHttpSlowingDown group_name:HAproxy severity:warning]"

- interval: 1m
  input_series:
  - series: 'haproxy_backend_http_total_time_average_seconds'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyHttpSlowingDown
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_backend_retry_warnings_total'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyRetryHigh
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy retry high (instance )"
        description: "High rate of retry on / backend\n  VALUE = 20\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyRetryHigh group_name:HAproxy severity:warning]"

- interval: 1m
  input_series:
  - series: 'haproxy_backend_retry_warnings_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyRetryHigh
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_frontend_requests_denied_total'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyFrontendSecurityBlockedRequests
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy frontend security blocked requests (instance )"
        description: "HAProxy is blocking requests for security reason\n  VALUE = 20\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyFrontendSecurityBlockedRequests group_name:HAproxy severity:warning]"

- interval: 1m
  input_series:
  - series: 'haproxy_frontend_requests_denied_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: HAproxy
    alertname: HaproxyFrontendSecurityBlockedRequests
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_backend_up'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyBackendDown
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy backend down (instance )"
        description: "HAProxy backend is down\n  VALUE = 0\n  LABELS: map[__name__:haproxy_backend_up alertgroup:HAproxy alertname:HaproxyBackendDown group_name:HAproxy severity:critical]"

- interval: 1m
  input_series:
  - series: 'haproxy_backend_up'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyBackendDown
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'haproxy_server_up'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyServerDown
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy server down (instance )"
        description: "HAProxy server is down\n  VALUE = 0\n  LABELS: map[__name__:haproxy_server_up alertgroup:HAproxy alertname:HaproxyServerDown group_name:HAproxy severity:critical]"

- interval: 1m
  input_series:
  - series: 'haproxy_server_up'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyServerDown
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'haproxy_server_check_failures_total'
    values: "1+1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyServerHealthcheckFailure
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: HAproxy
      exp_annotations:
        summary: "HAProxy server healthcheck failure (instance )"
        description: "Some server healthcheck are failing on \n  VALUE = 5\n  LABELS: map[alertgroup:HAproxy alertname:HaproxyServerHealthcheckFailure group_name:HAproxy severity:warning]"

- interval: 1m
  input_series:
  - series: 'haproxy_server_check_failures_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: HAproxy
    alertname: HaproxyServerHealthcheckFailure
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="NotOK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+2x1440"
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="OK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+100x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: Etcd
    alertname: EtcdWarningNumberOfFailedGrpcRequests
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
        grpc_method: NotWatch
        grpc_service: test
      exp_annotations:
        summary: "Etcd high number of failed GRPC requests (instance )"
        description: "More than 1 percent GRPC request failure detected in Etcd for 5 minutes\n  VALUE = 0.019607843137254898\n  LABELS: map[alertgroup:Etcd alertname:EtcdWarningNumberOfFailedGrpcRequests group_name:Etcd grpc_method:NotWatch grpc_service:test severity:warning]"

- interval: 1m
  input_series:
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="NotOK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+0x1440"
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="OK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+100x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: Etcd
    alertname: EtcdWarningNumberOfFailedGrpcRequests
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="NotOK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+6x1440"
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="OK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+100x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: Etcd
    alertname: EtcdCriticalNumberOfFailedGrpcRequests
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: Etcd
        grpc_method: NotWatch
        grpc_service: test
      exp_annotations:
        summary: "Etcd high number of failed GRPC requests (instance )"
        description: "More than 5 percent GRPC request failure detected in Etcd for 5 minutes\n  VALUE = 0.05660377358490566\n  LABELS: map[alertgroup:Etcd alertname:EtcdCriticalNumberOfFailedGrpcRequests group_name:Etcd grpc_method:NotWatch grpc_service:test severity:critical]"

- interval: 1m
  input_series:
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="NotOK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+0x1440"
  - series: 'grpc_server_handled_total{job="etcd",grpc_code="OK", grpc_method="NotWatch", grpc_service="test"}'
    values: "0+100x1440"
  alert_rule_test:
  - eval_time: 6m
    groupname: Etcd
    alertname: EtcdCriticalNumberOfFailedGrpcRequests
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.05"}'
    values: "0+1x1440"
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.1"}'
    values: "0+2x1440"
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.15"}'
    values: "0+3x1440"
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.2"}'
    values: "0+1000x1440"    
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="+Inf"}'
    values: "0+2000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdGrpcRequestsSlow
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
      exp_annotations:
        summary: "Etcd GRPC requests slow (instance )"
        description: "GRPC requests slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = 0.2\n  LABELS: map[alertgroup:Etcd alertname:EtcdGrpcRequestsSlow group_name:Etcd severity:warning]"

- interval: 1m
  input_series:
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.005"}'
    values: "0+1x1440"
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.01"}'
    values: "0+2x1440"
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.015"}'
    values: "0+3x1440"
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="0.02"}'
    values: "0+1000x1440"    
  - series: 'grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary", le="+Inf"}'
    values: "0+1001x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdGrpcRequestsSlow
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'etcd_server_has_leader'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: Etcd
    alertname: EtcdNoLeader
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: Etcd
      exp_annotations:
        summary: "Etcd no Leader (instance )"
        description: "Etcd cluster have no leader\n  VALUE = 0\n  LABELS: map[__name__:etcd_server_has_leader alertgroup:Etcd alertname:EtcdNoLeader group_name:Etcd severity:critical]"

- interval: 1m
  input_series:
  - series: 'etcd_server_has_leader'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: Etcd
    alertname: EtcdNoLeader
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'etcd_server_leader_changes_seen_total'
    values: "1+1x120"
  alert_rule_test:
  - eval_time: 2h
    groupname: Etcd
    alertname: EtcdHighNumberOfLeaderChanges
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
      exp_annotations:
        summary: "Etcd high number of leader changes (instance )"
        description: "Etcd leader changed more than 3 times during last hour\n  VALUE = 60\n  LABELS: map[alertgroup:Etcd alertname:EtcdHighNumberOfLeaderChanges group_name:Etcd severity:warning]"

- interval: 1m
  input_series:
  - series: 'etcd_server_leader_changes_seen_total'
    values: "0x120"
  alert_rule_test:
  - eval_time: 2h
    groupname: Etcd
    alertname: EtcdHighNumberOfLeaderChanges
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'etcd_server_proposals_failed_total'
    values: "1+1x120"
  alert_rule_test:
  - eval_time: 2h
    groupname: Etcd
    alertname: EtcdHighNumberOfFailedProposals
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
      exp_annotations:
        summary: "Etcd high number of failed proposals (instance )"
        description: "Etcd server got more than 5 failed proposals past hour\n  VALUE = 60\n  LABELS: map[alertgroup:Etcd alertname:EtcdHighNumberOfFailedProposals group_name:Etcd severity:warning]"

- interval: 1m
  input_series:
  - series: 'etcd_server_proposals_failed_total'
    values: "0x120"
  alert_rule_test:
  - eval_time: 2h
    groupname: Etcd
    alertname: EtcdHighNumberOfFailedProposals
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.1"}'
    values: "0+1x1440"
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.3"}'
    values: "0+2x1440"
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.5"}'
    values: "0+3x1440"
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="1"}'
    values: "0+1000x1440"    
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="+Inf"}'
    values: "0+2000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdHighFsyncDurations
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
      exp_annotations:
        summary: "Etcd high fsync durations (instance )"
        description: "Etcd WAL fsync duration increasing, 99th percentil is over 0.5s for 5 minutes\n  VALUE = 1\n  LABELS: map[alertgroup:Etcd alertname:EtcdHighFsyncDurations group_name:Etcd severity:warning]"

- interval: 1m
  input_series:
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.01"}'
    values: "0+1x1440"
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.03"}'
    values: "0+2x1440"
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.05"}'
    values: "0+3x1440"
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="0.1"}'
    values: "0+1000x1440"    
  - series: 'etcd_disk_wal_fsync_duration_seconds_bucket{le="+Inf"}'
    values: "0+1001x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdHighFsyncDurations
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.1"}'
    values: "0+1x1440"
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.3"}'
    values: "0+2x1440"
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.5"}'
    values: "0+3x1440"
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="1"}'
    values: "0+1000x1440"    
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="+Inf"}'
    values: "0+2000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdHighCommitDurations
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
      exp_annotations:
        summary: "Etcd high commit durations (instance )"
        description: "Etcd commit duration increasing, 99th percentil is over 0.25s for 5 minutes\n  VALUE = 1\n  LABELS: map[alertgroup:Etcd alertname:EtcdHighCommitDurations group_name:Etcd severity:warning]"

- interval: 1m
  input_series:
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.01"}'
    values: "0+1x1440"
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.03"}'
    values: "0+2x1440"
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.05"}'
    values: "0+3x1440"
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="0.1"}'
    values: "0+1000x1440"    
  - series: 'etcd_disk_backend_commit_duration_seconds_bucket{le="+Inf"}'
    values: "0+1001x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdHighCommitDurations
    exp_alerts: []

- interval: 5m
  input_series:
  - series: etcd_server_id {job="etcd", label1="test1"}
    values: "1x1440"
  - series: etcd_server_id {job="etcd", label2="test2"}
    values: "1x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: Etcd
    alertname: EtcdInsufficientMembers
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: Etcd
      exp_annotations:
        summary: "Etcd insufficient Members (instance )"
        description: "Etcd cluster should have an odd number of members\n  VALUE = 0\n  LABELS: map[alertgroup:Etcd alertname:EtcdInsufficientMembers group_name:Etcd severity:critical]"

- interval: 5m
  input_series:
  - series: etcd_server_id {job="etcd"}
    values: "1x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: Etcd
    alertname: EtcdInsufficientMembers
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="1"}'
    values: "0+1x1440"
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="2"}'
    values: "0+2x1440"
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="3"}'
    values: "0+3x1440"
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="5"}'
    values: "0+100x1440"    
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="+Inf"}'
    values: "0+1000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdMemberCommunicationSlow
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: Etcd
        job: etcd
      exp_annotations:
        summary: "Etcd member communication slow (instance )"
        description: "Etcd member communication slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = 5\n  LABELS: map[alertgroup:Etcd alertname:EtcdMemberCommunicationSlow group_name:Etcd job:etcd severity:warning]"

- interval: 1m
  input_series:
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="0.01"}'
    values: "0+100x1440"
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="0.02"}'
    values: "0+200x1440"
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="0.03"}'
    values: "0+300x1440"
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="0.05"}'
    values: "0+400x1440"    
  - series: 'etcd_network_peer_round_trip_time_seconds_bucket{job="etcd", le="+Inf"}'
    values: "0+401x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: Etcd
    alertname: EtcdMemberCommunicationSlow
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'nginx_ingress_controller_requests{status="400"}'
    values: "130+130x1440"
  - series: 'nginx_ingress_controller_requests'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: NginxIngressAlerts
    alertname: NginxHighHttp4xxErrorRate
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NginxIngressAlerts
      exp_annotations:
        summary: "Nginx high HTTP 4xx error rate (node: , namespace: , ingress: )"
        description: "Too many HTTP requests with status 4xx (> 5 percent)\n  VALUE = 9.774436090225564\n  LABELS = map[alertgroup:NginxIngressAlerts alertname:NginxHighHttp4xxErrorRate group_name:NginxIngressAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'nginx_ingress_controller_requests{status="400"}'
    values: "0x1440"
  - series: 'nginx_ingress_controller_requests'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: NginxIngressAlerts
    alertname: NginxHighHttp4xxErrorRate
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'nginx_ingress_controller_requests{status="500"}'
    values: "130+130x1440"
  - series: 'nginx_ingress_controller_requests'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: NginxIngressAlerts
    alertname: NginxHighHttp5xxErrorRate
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NginxIngressAlerts
      exp_annotations:
        summary: "Nginx high HTTP 5xx error rate (node: , namespace: , ingress: )"
        description: "Too many HTTP requests with status 5xx (> 5 percent)\n  VALUE = 9.774436090225564\n  LABELS = map[alertgroup:NginxIngressAlerts alertname:NginxHighHttp5xxErrorRate group_name:NginxIngressAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'nginx_ingress_controller_requests{status="500"}'
    values: "0x1440"
  - series: 'nginx_ingress_controller_requests'
    values: "1200+1200x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: NginxIngressAlerts
    alertname: NginxHighHttp5xxErrorRate
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="1"}'
    values: "0+1x1440"
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="2"}'
    values: "0+2x1440"
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="3"}'
    values: "0+3x1440"
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="5"}'
    values: "0+100x1440"    
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+1000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: NginxIngressAlerts
    alertname: NginxLatencyHigh
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: NginxIngressAlerts
      exp_annotations:
        summary: "Nginx latency high (node: , host: )"
        description: "Nginx p99 latency is higher than 3 seconds\n  VALUE = 5\n  LABELS = map[alertgroup:NginxIngressAlerts alertname:NginxLatencyHigh group_name:NginxIngressAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="0.1"}'
    values: "0+1x1440"
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="0.2"}'
    values: "0+2x1440"
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="0.3"}'
    values: "0+3x1440"
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="0.5"}'
    values: "0+100x1440"    
  - series: 'nginx_ingress_controller_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+101x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: NginxIngressAlerts
    alertname: NginxLatencyHigh
    exp_alerts: []
       
- interval: 1m
  input_series:
  - series: 'coredns_panics_total'
    values: "1+1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: CoreDnsAlerts
    alertname: CorednsPanicCount
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS Panic Count (instance )"
        description: "Number of CoreDNS panics encountered\n  VALUE = 1\n  LABELS = map[alertgroup:CoreDnsAlerts alertname:CorednsPanicCount group_name:CoreDnsAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'coredns_panics_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: CoreDnsAlerts
    alertname: CorednsPanicCount
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_dns_request_duration_seconds_bucket{le="1"}'
    values: "0+1x1440"
  - series: 'coredns_dns_request_duration_seconds_bucket{le="2"}'
    values: "0+2x1440"
  - series: 'coredns_dns_request_duration_seconds_bucket{le="3"}'
    values: "0+3x1440"
  - series: 'coredns_dns_request_duration_seconds_bucket{le="5"}'
    values: "0+100x1440"    
  - series: 'coredns_dns_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+1000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: CoreDnsAlerts
    alertname: CoreDNSLatencyHigh
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS have High Latency"
        description: "CoreDNS has 99th percentile latency of 5 seconds for server  zone "

- interval: 1m
  input_series:
  - series: 'coredns_dns_request_duration_seconds_bucket{le="0.1"}'
    values: "0+1x1440"
  - series: 'coredns_dns_request_duration_seconds_bucket{le="0.2"}'
    values: "0+2x1440"
  - series: 'coredns_dns_request_duration_seconds_bucket{le="0.3"}'
    values: "0+3x1440"
  - series: 'coredns_dns_request_duration_seconds_bucket{le="0.5"}'
    values: "0+100x1440"    
  - series: 'coredns_dns_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+101x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: CoreDnsAlerts
    alertname: CoreDNSLatencyHigh
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_forward_healthcheck_broken_total'
    values: "1+240x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardHealthcheckFailureCount
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS health checks have failed to upstream server"
        description: "CoreDNS health checks have failed to upstream server "

- interval: 1m
  input_series:
  - series: 'coredns_forward_healthcheck_broken_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardHealthcheckFailureCount
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_forward_healthcheck_broken_total'
    values: "1+240x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardHealthcheckBrokenCount
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS health checks have failed for all upstream servers"
        description: "CoreDNS health checks failed for all upstream servers LABELS = map[alertgroup:CoreDnsAlerts alertname:CoreDNSForwardHealthcheckBrokenCount group_name:CoreDnsAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'coredns_forward_healthcheck_broken_total'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardHealthcheckBrokenCount
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_dns_responses_total{rcode="SERVFAIL"}'
    values: "1+23x1440"
  - series: 'coredns_dns_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSErrorsCritical
    exp_alerts:
    - exp_labels:
        severity: critical 
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS is returning SERVFAIL"
        description: "CoreDNS is returning SERVFAIL for 8.745% of requests"

- interval: 1m
  input_series:
  - series: 'coredns_dns_responses_total{rcode="SERVFAIL"}'
    values: "0x1440"
  - series: 'coredns_dns_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSErrorsCritical
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_dns_responses_total{rcode="SERVFAIL"}'
    values: "1+3x1440"
  - series: 'coredns_dns_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSErrorsWarning
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS is returning SERVFAIL"
        description: "CoreDNS is returning SERVFAIL for 1.235% of requests"

- interval: 1m
  input_series:
  - series: 'coredns_dns_responses_total{rcode="SERVFAIL"}'
    values: "0x1440"
  - series: 'coredns_dns_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSErrorsWarning
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_forward_request_duration_seconds_bucket{le="1"}'
    values: "0+1x1440"
  - series: 'coredns_forward_request_duration_seconds_bucket{le="2"}'
    values: "0+2x1440"
  - series: 'coredns_forward_request_duration_seconds_bucket{le="3"}'
    values: "0+3x1440"
  - series: 'coredns_forward_request_duration_seconds_bucket{le="5"}'
    values: "0+100x1440"    
  - series: 'coredns_forward_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+1000x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardLatencyHigh
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS has 99th percentile latency for forwarding requests"
        description: "CoreDNS has 99th percentile latency of 5 seconds forwarding requests to "

- interval: 1m
  input_series:
  - series: 'coredns_forward_request_duration_seconds_bucket{le="0.1"}'
    values: "0+1x1440"
  - series: 'coredns_forward_request_duration_seconds_bucket{le="0.2"}'
    values: "0+2x1440"
  - series: 'coredns_forward_request_duration_seconds_bucket{le="0.3"}'
    values: "0+3x1440"
  - series: 'coredns_forward_request_duration_seconds_bucket{le="0.5"}'
    values: "0+100x1440"    
  - series: 'coredns_forward_request_duration_seconds_bucket{le="+Inf"}'
    values: "0+101x1440"
  alert_rule_test:
  - eval_time: 30m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardLatencyHigh
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_forward_responses_total{rcode="SERVFAIL"}'
    values: "1+23x1440"
  - series: 'coredns_forward_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardErrorsCritical
    exp_alerts:
    - exp_labels:
        severity: critical 
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS is returning SERVFAIL for forward requests"
        description: "CoreDNS is returning SERVFAIL for 8.745% of forward requests to "

- interval: 1m
  input_series:
  - series: 'coredns_forward_responses_total{rcode="SERVFAIL"}'
    values: "0x1440"
  - series: 'coredns_forward_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardErrorsCritical
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'coredns_forward_responses_total{rcode="SERVFAIL"}'
    values: "1+3x1440"
  - series: 'coredns_forward_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardErrorsWarning
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: CoreDnsAlerts
      exp_annotations:
        summary: "CoreDNS is returning SERVFAIL for forward requests"
        description: "CoreDNS is returning SERVFAIL for 1.235% of forward requests to "

- interval: 1m
  input_series:
  - series: 'coredns_forward_responses_total{rcode="SERVFAIL"}'
    values: "0x1440"
  - series: 'coredns_forward_responses_total'
    values: "1+240x1440"    
  alert_rule_test:
  - eval_time: 15m
    groupname: CoreDnsAlerts
    alertname: CoreDNSForwardErrorsWarning
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'probe_success'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: ProbeFailed
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: DRAlerts
      exp_annotations:
        summary: "Probe failed (instance: )"
        description: "Probe failed\n  VALUE = 0\n  LABELS: map[__name__:probe_success alertgroup:DRAlerts alertname:ProbeFailed group_name:DRAlerts severity:critical]"
        
- interval: 1m
  input_series:
  - series: 'probe_success'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: ProbeFailed
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'probe_duration_seconds'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: SlowProbe
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: DRAlerts
      exp_annotations:
        summary: "Slow probe (instance: )"
        description: "Blackbox probe took more than 1s to complete\n  VALUE = 2\n  LABELS: map[__name__:probe_duration_seconds alertgroup:DRAlerts alertname:SlowProbe group_name:DRAlerts severity:warning]"       

- interval: 1m
  input_series:
  - series: 'probe_duration_seconds'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: SlowProbe
    exp_alerts: []
        
- interval: 1m
  input_series:
  - series: 'probe_http_status_code'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: HttpStatusCode
    exp_alerts:
    - exp_labels:
        severity: critical
        group_name: DRAlerts
      exp_annotations:
        summary: "HTTP Status Code (instance: )"
        description: "HTTP status code is not 200-399\n  VALUE = 0\n  LABELS: map[__name__:probe_http_status_code alertgroup:DRAlerts alertname:HttpStatusCode group_name:DRAlerts severity:critical]"

- interval: 1m
  input_series:
  - series: 'probe_http_status_code'
    values: "200x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: HttpStatusCode
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'probe_http_duration_seconds'
    values: "2x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: HttpSlowRequests
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: DRAlerts
      exp_annotations:
        summary: "HTTP slow requests (instance: )"
        description: "HTTP request took more than 1s\n  VALUE = 2\n  LABELS: map[__name__:probe_http_duration_seconds alertgroup:DRAlerts alertname:HttpSlowRequests group_name:DRAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'probe_http_duration_seconds'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: DRAlerts
    alertname: HttpSlowRequests
    exp_alerts: []

- interval: 1m
  input_series:
  - series: 'backup_storage_last_failed'
    values: "1x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: BackupAlerts
    alertname: Last Backup Failed
    exp_alerts:
    - exp_labels:
        severity: warning
        group_name: BackupAlerts
      exp_annotations:
        summary: "Last backup made by pod  in namespace  failed.\n  VALUE = 1\n  LABELS: map[__name__:backup_storage_last_failed alertgroup:BackupAlerts alertname:Last Backup Failed group_name:BackupAlerts severity:warning]"
        description: "Last backup made by pod  in namespace  failed.\n  VALUE = 1\n  LABELS: map[__name__:backup_storage_last_failed alertgroup:BackupAlerts alertname:Last Backup Failed group_name:BackupAlerts severity:warning]"

- interval: 1m
  input_series:
  - series: 'backup_storage_last_failed'
    values: "0x1440"
  alert_rule_test:
  - eval_time: 5m
    groupname: BackupAlerts
    alertname: Last Backup Failed
    exp_alerts: []